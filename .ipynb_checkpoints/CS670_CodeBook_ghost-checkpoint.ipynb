{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#packages and libraries\n",
    "import os,sys\n",
    "from os import listdir\n",
    "import string\n",
    "import re\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import operator\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining MACROS and Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Loading the AoTM data\n",
      "Total Playlist found:  29164\n"
     ]
    }
   ],
   "source": [
    "#Global Vars and Macros\n",
    "#_BASE_DIRECTORY = '/Users/007ri/Miniconda2/envs/onedropcs670/Datasets/AoTM'\n",
    "_BASE_DIRECTORY = '/Users/sidverma/anaconda/Project/AoTM'\n",
    "_NN = 300\n",
    "_SPLIT_RATIO = 0.8\n",
    "dirs = os.listdir(_BASE_DIRECTORY)\n",
    "\n",
    "filename = 'aotm_list_ids.txt'\n",
    "currURL = _BASE_DIRECTORY+'/'+filename\n",
    "response = urllib.urlopen(currURL)\n",
    "lines = response.readlines()\n",
    "AOTM_playLists = []\n",
    "for line in lines:\n",
    "    AOTM_playLists += line,\n",
    "print 'Done Loading the AoTM data'\n",
    "print 'Total Playlist found: ', len(AOTM_playLists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the pre-processing of AOTM Dataset [..add more later]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with processing an parsing the lists. Check stats below: \n",
      "Number of Playlists:  29164\n",
      "Total Songs in the AoTM dataset:  218261\n"
     ]
    }
   ],
   "source": [
    "#This is the playList class and structure. We will be using this as out playList Object to store and manipulate playlist.\n",
    "#self._pid : playlist ID\n",
    "#self._songList : list of tuples of (ArtistID, SongID)\n",
    "#self._songFreqDict : the frequency table of the songs only in the current playlist\n",
    "#self._songFreqDictNorm : _songFreqDict in the nomralized form\n",
    "class playList():\n",
    "    def __init__(self, inPid, inSongList, inSongDict = {}, inSongNormDict = {}, inTrainList = [], inTestList = []):\n",
    "        self._pid = inPid # playlist ID\n",
    "        self._songList = inSongList #list of tuples of (ArtistID, SongID)\n",
    "        self._songFreqDict = inSongDict\n",
    "        self._songFreqDictNorm = inSongNormDict\n",
    "        self._trainList = inTrainList\n",
    "        self._testList = inTestList\n",
    "    def __str__(self):\n",
    "        strObj = 'ID: '+str(self._pid)+'\\n'\n",
    "        for tup in self._songList:\n",
    "            strObj += str(tup[0]) + ': '\n",
    "            strObj += str(tup[1]) + '\\n'\n",
    "        return strObj\n",
    "    def setSongDict(self, inDict):\n",
    "        self._songFreqDict = inDict\n",
    "    def setSongDictNorm(self, inNormDict):\n",
    "        self._songFreqDictNorm = inNormDict\n",
    "    def setTrainList(self, inTrainList):\n",
    "        self._trainList = inTrainList\n",
    "    def setTestList(self, inTestList):\n",
    "        self._testList = inTestList\n",
    "\n",
    "##########################################################################################################################\n",
    "#This is a helper finctions sections. {[rishabh] Please put all your helper functions here for the modularity of the code}\n",
    "##########################################################################################################################\n",
    "\n",
    "#genVocab():\n",
    "#Purpose: to generate the entire vocablulary.\n",
    "#Input paramas: the input params are the entire dataset which is in the form of list of playList objects.\n",
    "#Return Value: The return is the set of the unique songs in the entire input data set\n",
    "def genVocab(inData):\n",
    "    totalSongsInputSpace = set()\n",
    "    for pl in inData:\n",
    "        for tup in pl._songList:\n",
    "            totalSongsInputSpace.add(tup[1])\n",
    "    return totalSongsInputSpace\n",
    "\n",
    "##########################################################################################################################    \n",
    "#cleanNSerializePlaylist():\n",
    "#Purpose: to clean, organize and restructure the input playlist which is in string format.\n",
    "#Input paramas: the input params are the playList in the string format (raw form as read from the aotm dataset).\n",
    "#Return Value: The return is playList Object of the input playlist\n",
    "def cleanNSerializePlaylist(inDataEntry):\n",
    "    ltmp = inDataEntry.split()\n",
    "    playListID = ltmp[0]\n",
    "    ltmp.pop(0)\n",
    "    playListID = re.sub('#', '', playListID)\n",
    "    n = len(ltmp)\n",
    "    songList = []\n",
    "    for i in xrange(0,n-1,2):\n",
    "        artistid = ltmp[i][:-1]\n",
    "        songid = ltmp[i+1]\n",
    "        tup = (artistid,songid)\n",
    "        songList.append(tup)\n",
    "    retObj = playList(playListID, songList)\n",
    "    return retObj\n",
    "\n",
    "##########################################################################################################################    \n",
    "#reStructureAOTMDataset():\n",
    "#Purpose: a wrapper around the cleanNSerializePlaylist() to call it on every playList in the dataset.\n",
    "#Input paramas: the input params are the entire dataset which is in the form of list of playList as read from the aotm dataset.\n",
    "#Return Value: The return is list of playList Object.\n",
    "def reStructureAOTMDataset(inData):\n",
    "    retVal = []\n",
    "    allVocab = inData\n",
    "    for entry in inData:\n",
    "        retVal.append(cleanNSerializePlaylist(entry))\n",
    "    return retVal\n",
    "\n",
    "##########################################################################################################################    \n",
    "#pidPlObjMapper():\n",
    "#Purpose: a wrapper around the cleanNSerializePlaylist() to call it on every playList in the dataset.\n",
    "#Input paramas: the input params is list of playList Object.\n",
    "#Return Value: The return is a dictionary of pid : playList obj.\n",
    "def pidPlObjMapper(inData):\n",
    "    retDict = collections.defaultdict()\n",
    "    for plObj in inData:\n",
    "        retDict[plObj._pid] = plObj\n",
    "    return retDict\n",
    "\n",
    "##########################################################################################################################    \n",
    "#AOTM_CleanDataSet :- Contains all playList object of every playlist inthe aotm dataset. Look at the class playList() for details of object structure\n",
    "#AOTM_All_Songs :- Contains all the songs ever present in the entire aotm dataset\n",
    "#AOTM_pid_plObj_mapping :- Contains the {pid : playList object} mapping.\n",
    "\n",
    "AOTM_CleanDataSet = reStructureAOTMDataset(AOTM_playLists)\n",
    "AOTM_All_Songs = genVocab(AOTM_CleanDataSet)\n",
    "AOTM_pid_plObj_mapping = pidPlObjMapper(AOTM_CleanDataSet)\n",
    "\n",
    "print 'Done with processing an parsing the lists. Check stats below: '\n",
    "print 'Number of Playlists: ', len(AOTM_CleanDataSet)\n",
    "print 'Total Songs in the AoTM dataset: ', len(AOTM_All_Songs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "* Let's look at some of the statistical aspects of our datasets\n",
    "* We hope to gain some valuable insights which might help our analysis in terms of cleaning the dataset to remove outliers and other spurious features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhIAAAFkCAYAAAB1rtL+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x103f53410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##########################################################################################################################    \n",
    "#analysisPlaylistSize():\n",
    "#Purpose: an analysis function to look at the distribution of Playlist Lengths.\n",
    "#Input paramas: the input params is list of playList Object.\n",
    "#Return Value: The return is null.\n",
    "\n",
    "def analysisPlaylistSize(inData):\n",
    "    X , Y = [], []\n",
    "    temp = []\n",
    "    for p in inData:\n",
    "        temp.append(len(p._songList))\n",
    "    temp.sort()\n",
    "    idx = 1\n",
    "    for t in temp:\n",
    "        X.append(idx)\n",
    "        Y.append(t)\n",
    "        idx += 1\n",
    "    plt.plot(X, Y)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    return \n",
    "analysisPlaylistSize(AOTM_CleanDataSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways:\n",
    "* As we can see from the above analyses, approx ~ 2000 playlists have a length fewer than 10 songs. Hence it may be difficult to create good recommendations for such user playlists since the vector size is relatively smaller than most of the others in the dataset.\n",
    "* Thus we propose, to select only those playlists that have a vector length > 10 songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################################################################################################    \n",
    "#analysisTopTags():\n",
    "#Purpose: an analysis function to look at the distribution of most Popular Tags.\n",
    "#Input paramas: the input params is list of playList Object.\n",
    "#Return Value: The return is null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done removing unwanted playLists, viz pl with <10 songs!\n",
      "Done setting train and test lists!\n",
      "Done Creating the song freq dicts!\n",
      "Done Creating the NORMALIZED song freq dicts!\n"
     ]
    }
   ],
   "source": [
    "##########################################################################################################################    \n",
    "#removeUnwantedPlaylist():\n",
    "#Purpose: remove the playLists with less k songs.\n",
    "#Input paramas: the input params is list of playList Object & k -> thresHold.\n",
    "#Return Value: list of playList Object with >k songs.\n",
    "def removeUnwantedPlaylist(inData, k = 10):\n",
    "    retval = []\n",
    "    for i in inData:\n",
    "        if len(i._songList) < 10:\n",
    "            continue\n",
    "        retval.append(i)\n",
    "    return retval\n",
    "\n",
    "#Clean data\n",
    "AOTM_CleanDataSet = removeUnwantedPlaylist(AOTM_CleanDataSet, 10)\n",
    "print \"Done removing unwanted playLists, viz pl with <10 songs!\"\n",
    "##########################################################################################################################    \n",
    "#setTestTrainLists():\n",
    "#Purpose: a helper to set the plObj._trainList, plObj._testList\n",
    "#Input paramas: the input params is list of playList Object.\n",
    "#Return Value: void.\n",
    "def setTestTrainLists(inData):\n",
    "    for pl in inData:\n",
    "        n = len(pl._songList)\n",
    "        k = int(_SPLIT_RATIO*n)\n",
    "        trainList = pl._songList[:k]\n",
    "        testList = pl._songList[k:]\n",
    "        pl.setTrainList(trainList)\n",
    "        pl.setTestList(testList)\n",
    "    return\n",
    "#invoke fucntion\n",
    "setTestTrainLists(AOTM_CleanDataSet)\n",
    "print \"Done setting train and test lists!\"\n",
    "##########################################################################################################################    \n",
    "#setVocabDicts():\n",
    "#Purpose: a helper to set the plObj.__songFreqDict for each playlist.\n",
    "#Input paramas: the input params is list of playList Object.\n",
    "#Return Value: void.\n",
    "def setVocabDicts(inData):\n",
    "    for pl in inData:\n",
    "        plVecDict = collections.defaultdict(int)\n",
    "        for tup in pl._trainList:\n",
    "            plVecDict[tup[1]] += 1\n",
    "        pl.setSongDict(plVecDict)\n",
    "    return\n",
    "\n",
    "#invoking the above function call\n",
    "setVocabDicts(AOTM_CleanDataSet)\n",
    "print \"Done Creating the song freq dicts!\"\n",
    "\n",
    "##########################################################################################################################    \n",
    "#normalize():\n",
    "#Purpose: a helper to normalize the plObj._songFreqDict for each playlist.\n",
    "#Input paramas: the input params is playList._songFreqDict.\n",
    "#Return Value: the normalized dict of the input playList.\n",
    "def normalize(inPlaylistDict):\n",
    "    normSum = 0.0\n",
    "    retDict = collections.defaultdict()\n",
    "    for i in inPlaylistDict:\n",
    "        normSum += inPlaylistDict[i]*inPlaylistDict[i]\n",
    "    normSum = normSum**0.5\n",
    "    for i in inPlaylistDict:\n",
    "        retDict[i] = (1.0*inPlaylistDict[i])/normSum\n",
    "    return retDict\n",
    "\n",
    "##########################################################################################################################    \n",
    "#setNormVocabDicts():\n",
    "#Purpose: a wrapper to set the plObj._songFreqDictNorm for each playlist.\n",
    "#Input paramas: the input params is list of playList Object.\n",
    "#Return Value: void\n",
    "def setNormVocabDicts(inData):\n",
    "    for pl in inData:\n",
    "        SongFreqDictNorm = normalize(pl._songFreqDict)\n",
    "        pl.setSongDictNorm(SongFreqDictNorm)\n",
    "    return\n",
    "\n",
    "#invoking the above function call\n",
    "setNormVocabDicts(AOTM_CleanDataSet)\n",
    "print \"Done Creating the NORMALIZED song freq dicts!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Vector Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Done!!!!\n"
     ]
    }
   ],
   "source": [
    "##########################################################################################################################    \n",
    "#getCosineScore():\n",
    "#Purpose: to calculate the cosine similarity .\n",
    "#Input paramas: the input params are two playList Objects whose similarity is to be calculated.\n",
    "#Return Value: cosine similarity measure (float value)\n",
    "def getCosineScore(inPlaylist, inRefPlayList):\n",
    "    score = 0.0\n",
    "    inPlaylistDict = inPlaylist._songFreqDictNorm\n",
    "    inRefPlayListDict = inRefPlayList._songFreqDictNorm\n",
    "    for i in inPlaylistDict:\n",
    "        if i in inRefPlayListDict:\n",
    "            score += (1.0*inPlaylistDict[i]*inRefPlayListDict[i])\n",
    "    return score\n",
    "\n",
    "##########################################################################################################################    \n",
    "#cosineSimilartiy():\n",
    "#Purpose: to calculate the cosine similarity .\n",
    "#Input paramas: the input params is list of playList Object in the entrie input space and the user playlist.\n",
    "#Return Value: a dict of playList ids as with similarity measure as value.\n",
    "def cosineSimilartiy(inPlayList, inOtherPlayLists):\n",
    "    res = collections.defaultdict(float)\n",
    "    for refPl in inOtherPlayLists:\n",
    "        if refPl._pid == inPlayList._pid:\n",
    "            continue\n",
    "        cosineScore = getCosineScore(inPlayList, refPl)\n",
    "        res[refPl._pid] = cosineScore\n",
    "    return res\n",
    "\n",
    "##########################################################################################################################    \n",
    "#getKNN():\n",
    "#Purpose: get k nearest neighbors to inPut playlist.(Distance sim)\n",
    "#Input paramas: the input params is user playList, list of playList Object and inK which is defaulted to _NN (macro defined at the beginng).\n",
    "#Return Value: top inK number of playlist IDs as a list.\n",
    "def getKNN(inPlayList, inOtherPlayLists, bTruncateZeros = False, inK = _NN):#_NN is a macro define above with value 300 or less if we have <300 similars.\n",
    "    simlarityDict = cosineSimilartiy(inPlayList, inOtherPlayLists)\n",
    "    sorted_simlarityDict = sorted(simlarityDict.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    retVal = []\n",
    "    for kv in sorted_simlarityDict[:inK]:\n",
    "        if bTruncateZeros and kv[1] <= 0.0:\n",
    "            return retVal\n",
    "        retVal += kv[0],\n",
    "    return retVal\n",
    "\n",
    "inP = AOTM_CleanDataSet[0]\n",
    "inO = AOTM_CleanDataSet\n",
    "tmp = getKNN(inP, inO, True)\n",
    "print len(tmp)\n",
    "print \"Done!!!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory of why not to use aotm dataset and moving on to lastfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Include all you Packages and Libs here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#packages and libraries\n",
    "import os,sys\n",
    "import string\n",
    "import re\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Include your globar vars and MACROS here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Put your base directory here\n",
    "#ADD ALL YOUR FILE NAMES HERE AND USE THESE NAMES IN THE DICT\n",
    "_BASEDIRECTORY = '/Users/sidverma/Desktop/IR/'\n",
    "_FILE_MAIN = 'Final.csv'\n",
    "_FILE_TAG_FREQ_FILE = 'FinalTagFreq'\n",
    "_FILE_SONG_BOOL_TAGS = 'FinalTagroomMergedVecs'\n",
    "_FILE_SAMPLE_USERS = 'Best_Users.txt'\n",
    "_FILE_KNN_BASELINE_USERS = 'knnSimilarity.csv'\n",
    "_USER_LIST = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# last.fm Dataset Crawling and Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using pylast module to crawl data exposed by Last.fm.  pylast is a Python interface to Last.fm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Getting usernames for users\n",
    "We started by collecting 6000 usernames in their database. We started by one user and added his friends to the user-list. We did this in Breadth First Search Pattern. This was a challenging task considering the network errors in the API calls and amount of time required. We finally used on 3000 users out of 6000 users.\n",
    "## b) Getting user playlists based on usernames\n",
    "Next for all the usernames in the user list obtained in the previous step, we found the last 200 songs listened by them. We consider that as a playlist for that user. We collected information like \"\"\n",
    "## c) Getting song meta-data like social tags\n",
    "Next we collected social tags for each song fetched in the previous step. We observed that the tag data was skewed based on the the number of tags. So we cleaned the tags later on. The tag information is used in the content-based scoring.\n",
    "## d) Getting song numerical features like popularity\n",
    "For each song we also found numerical features like total listeners count and number of times the track was played. This data is used in scoring based on Numerical Features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis and Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation and decision based on them moving fwrd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faceted Track Scoring\n",
    "In this section, we present the details of our multi-faceted\n",
    "weighted scoring approach. The goal is to determine a relevance score for each possible next track given a playlist history h (sequence of tracks) using different input signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['monkeyhacker', 'badboy495', 'sonnycorleones', 'heavydirtysoul_', 'Garry_Drezden']\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Helper\n",
    "os.chdir(_BASEDIRECTORY)\n",
    "def getUsers():\n",
    "    f = open(_FILE_SAMPLE_USERS, 'r')\n",
    "    users = []\n",
    "    for l in f:\n",
    "        users.append(l[:-1])\n",
    "    return users\n",
    "_USER_LIST = getUsers()\n",
    "print _USER_LIST\n",
    "print \"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Baseline Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1A) Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used kNN Algorithm as a baseline. Given a history h and a set $N_{h}$ of nearest neighbor playlists of h, we compute the kNN score of a target track $t_{*}$ with $sim_{cosine}(h, n)$ as the binary cosine similarity of track occurrences in $h$ and $n$. $1_{n}(t^{*}) = 1$ if $n$ contains $t^{*}$ and 0 otherwise.\n",
    "\n",
    "$$score_{kNN}(h, t^{*}) = \\sum_{n \\epsilon N_{h}} sim_{cosine}(h,n)*1_{n}(t^{*}) $$\n",
    "\n",
    "This is done in  two steps:  \n",
    "i.) In first phase we calculate 300 nearest neighbors of the user based on cosine similarity of the user playlist and other users' plalylist. The scores are reported in file _FILE_KNN_BASELINE_USERS_.  \n",
    "ii.) In the next step, kNN baseline scores are calculated using the equation given above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1B[i]) Phase 1 Code: Getting Nearest Users using KNN300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_SPLIT_RATIO=0.8\n",
    "_NUM_OF_NEIGHBORS=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class kNNGenerator(object):\n",
    "    def __init__(self):\n",
    "        self.user_list=[]\n",
    "        self.song_list=[]\n",
    "        self.similarities = []\n",
    "        self.similar_neighbors = []\n",
    "        self.similarity_scores = []\n",
    "\n",
    "    def similarity_func(self,playlist1, playlist2):\n",
    "        set2 = set(playlist2)\n",
    "        count = [1 for song in playlist1 if song in set2]\n",
    "        return float(sum(count)) / math.sqrt(len(playlist1) * len(playlist2))\n",
    "\n",
    "\n",
    "    def getUserSongList(self):\n",
    "        frame = pd.read_csv(_FILE_MAIN)\n",
    "        frame.drop([\"Playtime\", \"Album\", \"Match\", \"Listeners\", \"Playcount\", \"Duration\", \"Tags\"], axis=1, inplace=\"True\")\n",
    "        grouped = frame.groupby(by=\"Users\")\n",
    "        user_groups = grouped.groups\n",
    "        for k in user_groups.keys():\n",
    "            self.user_list.append(k)\n",
    "            user_group = grouped.get_group(k)\n",
    "            self.song_list.append(user_group.Songs.tolist())\n",
    "\n",
    "    def getSimilarityScores(self):\n",
    "        len_user_list = len(self.user_list)\n",
    "        for i, user in enumerate(self.user_list):\n",
    "            print \"Calculating 300NN for User\", i\n",
    "            similarity = []\n",
    "            end_index = int(math.ceil(_SPLIT_RATIO * len(self.song_list[i])))\n",
    "            curr_user_playlist = self.song_list[i][:end_index]\n",
    "            print \"User details: \", user, len(curr_user_playlist)\n",
    "            for j in range(0, i):\n",
    "                print \"Parsing Row: \", j\n",
    "                similarity.append((self.user_list[j], self.similarity_func(curr_user_playlist, self.song_list[j])))\n",
    "            for j in range(i + 1, len_user_list):\n",
    "                print \"Parsing Row: \", j\n",
    "                similarity.append((self.user_list[j], self.similarity_func(curr_user_playlist, self.song_list[j])))\n",
    "            self.similarities.append(similarity)\n",
    "\n",
    "\n",
    "    def sortNeighbors(self):\n",
    "        for i in range(len(self.similarities)):\n",
    "            self.similarities[i].sort(key=lambda x: float(x[1]), reverse=True)\n",
    "            self.similarities[i] = self.similarities[i][:_NUM_OF_NEIGHBORS]\n",
    "\n",
    "\n",
    "\n",
    "    def saveScores(self):\n",
    "        for similarity in self.similarities:\n",
    "            curr_similar_neighbors = [similar[0] for similar in similarity]\n",
    "            curr_similarity_score = [similar[1] for similar in similarity]\n",
    "            self.similar_neighbors.append(curr_similar_neighbors)\n",
    "            self.similarity_scores.append(curr_similarity_score)\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            data={\"Users\": self.user_list, \"kNN\": self.similar_neighbors, \"Similarities\": self.similarity_scores},\n",
    "            columns=[\"Users\", \"kNN\", \"Similarities\"])\n",
    "        df.to_csv(_FILE_KNN_BASELINE_USERS, sep=',')\n",
    "\n",
    "\n",
    "def generatekNN():\n",
    "    kNNGeneratorObj= kNNGenerator()\n",
    "    kNNGeneratorObj.getUserSongList()\n",
    "    kNNGeneratorObj.getSimilarityScores()\n",
    "    kNNGeneratorObj.sortNeighbors()\n",
    "    kNNGeneratorObj.saveScores()\n",
    "    print \"Done\"\n",
    "\n",
    "\n",
    "generatekNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1B[ii]) Phase 2 Code: Calculating Baseline Score using KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genrating CORPUS\n",
      "Done CORPUS GENERATION\n",
      "monkeyhacker 32012\n",
      "------------\n",
      "Calculating INVERTED INDEX\n",
      "INVERTED INDEX Calculated\n",
      "------------\n",
      "Calculating SCORES  monkeyhacker\n",
      "SCORES Calculated  monkeyhacker\n",
      "------------\n",
      "Dumping in File monkeyhackerKNN_Score.csv\n",
      "Genrating CORPUS\n",
      "Done CORPUS GENERATION\n",
      "badboy495 31271\n",
      "------------\n",
      "Calculating INVERTED INDEX\n",
      "INVERTED INDEX Calculated\n",
      "------------\n",
      "Calculating SCORES  badboy495\n",
      "SCORES Calculated  badboy495\n",
      "------------\n",
      "Dumping in File badboy495KNN_Score.csv\n",
      "Genrating CORPUS\n",
      "Done CORPUS GENERATION\n",
      "sonnycorleones 29795\n",
      "------------\n",
      "Calculating INVERTED INDEX\n",
      "INVERTED INDEX Calculated\n",
      "------------\n",
      "Calculating SCORES  sonnycorleones\n",
      "SCORES Calculated  sonnycorleones\n",
      "------------\n",
      "Dumping in File sonnycorleonesKNN_Score.csv\n",
      "Genrating CORPUS\n",
      "Done CORPUS GENERATION\n",
      "heavydirtysoul_ 30103\n",
      "------------\n",
      "Calculating INVERTED INDEX\n",
      "INVERTED INDEX Calculated\n",
      "------------\n",
      "Calculating SCORES  heavydirtysoul_\n",
      "SCORES Calculated  heavydirtysoul_\n",
      "------------\n",
      "Dumping in File heavydirtysoul_KNN_Score.csv\n",
      "Genrating CORPUS\n",
      "Done CORPUS GENERATION\n",
      "Garry_Drezden 34650\n",
      "------------\n",
      "Calculating INVERTED INDEX\n",
      "INVERTED INDEX Calculated\n",
      "------------\n",
      "Calculating SCORES  Garry_Drezden\n",
      "SCORES Calculated  Garry_Drezden\n",
      "------------\n",
      "Dumping in File Garry_DrezdenKNN_Score.csv\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "def getCleanUserList(inUsers):\n",
    "    inUsers = inUsers.replace('\\\"', '')\n",
    "    inUsers = inUsers.replace('[', '')\n",
    "    inUsers = inUsers.replace(']', '')\n",
    "    inUsers = inUsers.replace('\\'', '')\n",
    "    return inUsers.split(', ')\n",
    "\n",
    "def getCleanScoreList(inScore):\n",
    "    retVal = []\n",
    "    inScore = inScore.replace('[', '')\n",
    "    inScore = inScore.replace(']', '')\n",
    "    tmpList = inScore.split(', ')\n",
    "    for i in tmpList:\n",
    "        retVal.append(float(i))\n",
    "    return retVal\n",
    "\n",
    "def getSongCorpus(inUsers):\n",
    "    os.chdir(_BASEDIRECTORY)\n",
    "    df = pd.read_csv(_FILE_MAIN)\n",
    "    groupFrame = df.groupby(by = 'Users')\n",
    "    corpus = set()\n",
    "    for usr in inUsers:\n",
    "        currFrame = groupFrame.get_group(usr)\n",
    "        songsPool = currFrame.Songs.tolist()\n",
    "        for s in songsPool:\n",
    "            corpus.add(s)\n",
    "    return corpus\n",
    "\n",
    "def calculateSongsInvertedIndex(inUserList, inCorpus):\n",
    "    invertedIndexDict = collections.defaultdict(list)\n",
    "    os.chdir(_BASEDIRECTORY)\n",
    "    df = pd.read_csv(_FILE_MAIN)\n",
    "    groupFrame = df.groupby(by='Users')\n",
    "    for user in inUserList:\n",
    "        currFrame = groupFrame.get_group(user)\n",
    "        userSongsList = currFrame.Songs.tolist()\n",
    "        for s in userSongsList:\n",
    "            invertedIndexDict[s].append(user)\n",
    "    return invertedIndexDict\n",
    "\n",
    "def getSongScore(song, inUser, occuringPlayLists, neighbor_scores_tuples_list):\n",
    "    retVal = 0.0\n",
    "    for tup in neighbor_scores_tuples_list:\n",
    "        neighbor = tup[0]\n",
    "        neighborScore = tup[1]\n",
    "        if neighbor in occuringPlayLists:\n",
    "            retVal += neighborScore\n",
    "    return retVal\n",
    "\n",
    "def calculateSimilarityMeasure(inUser, inInvertedScoresDict, neighbor_scores_tuples_list):\n",
    "    scoresDict = collections.defaultdict(float)\n",
    "    for song in inInvertedScoresDict:\n",
    "        score = getSongScore(song, inUser, inInvertedScoresDict[song], neighbor_scores_tuples_list)\n",
    "        scoresDict[song] = score\n",
    "    return scoresDict\n",
    "\n",
    "def knnSimilarity(inUserList):\n",
    "    os.chdir(_BASEDIRECTORY)\n",
    "    df = pd.read_csv(_FILE_KNN_BASELINE_USERS)\n",
    "    userFrame = df.groupby(by='Users')\n",
    "    index = 0\n",
    "    userAlreadyVisited = set()\n",
    "    for i, row in df.iterrows():\n",
    "        u = row['Users']\n",
    "        if u not in inUserList:\n",
    "            continue\n",
    "        if u in userAlreadyVisited:\n",
    "            continue\n",
    "        userAlreadyVisited.add(u)\n",
    "        neigbors = row['kNN']\n",
    "        userList = getCleanUserList(neigbors)\n",
    "        simScores = row['Similarities']\n",
    "        simScoresList = getCleanScoreList(simScores)\n",
    "        neighbor_scores_tuples_list = zip(userList, simScoresList)\n",
    "        print 'Genrating CORPUS'\n",
    "        currCorpus = getSongCorpus(userList)\n",
    "        print 'Done CORPUS GENERATION'\n",
    "        print u, len(currCorpus)\n",
    "        print '------------------------'\n",
    "        print 'Calculating INVERTED INDEX'\n",
    "        invertedScoresDict = calculateSongsInvertedIndex(userList, currCorpus)\n",
    "        print 'INVERTED INDEX Calculated'\n",
    "        print '------------------------'\n",
    "        print 'Calculating SCORES ', u\n",
    "        similarityScores = calculateSimilarityMeasure(u, invertedScoresDict, neighbor_scores_tuples_list)\n",
    "        print 'SCORES Calculated ', u\n",
    "        print '------------------------'\n",
    "        final_df = pd.DataFrame(similarityScores.items(), columns=['Songs', 'Score'])\n",
    "        fname = u + 'KNN_Score.csv'\n",
    "        print \"Dumping in File \" + fname\n",
    "        final_df.to_csv(fname)\n",
    "\n",
    "def entryPoint():\n",
    "    userList = getUsers()\n",
    "    knnSimilarity(_USER_LIST)\n",
    "    print 'DONE'\n",
    "\n",
    "entryPoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Additional Suitability Scores\n",
    "Playlist creators can have certain themes, goals or quality criteria in mind when they design a playlist. The idea of our faceted scoring method is to combine the kNN approach with additional suitability scores. If we, for example, detect that the homogeneity of the popularity of the tracks is most probably a guiding quality criterion, we should give an extra relevance weight to tracks that are similar in populairty to those in the history; if we observe that several tracks in the history were annotated by users with certain tags, we should increase the relevance score of tracks with similar tags. Generally, the combination of different scores shall serve\n",
    "two purposes: (1) increasing the hit rate as more relevant tracks receive a higher aggregated score and (2) making the playlist continuation more homogeneous, which is often a desirable goal in the Music Information Retrieval literature. In our experiments, we tested the following scores as examples to validate our approach. The selection was based on the availability of track metadata in public sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Content-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2A) Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieved the social tags  that were assigned to tracks by users on Last.fm as they are valuable indicators of what a certain playlist is about. We removed irrelevant tags like my favorite or like it, retained the 1,000 most frequent tags and computed TF-IDF (Term Frequency/Inverse Document Frequency) vectors for each track. Given a history $h$ consisting of tracks $t_{i}$ . . .  $t_{n}$ with the TF-IDF vectors $t_{1}^{v}$ to $t_{n}^{v}$, we compute the score of target track t^{*} using its TF-IDF vector t^{v} as follows. scorecontent(h, t) = simcosine\n",
    "$$score_{content}(h, t^{*}) = sim_{cosine}(\\sum_{t_{i}^{v}\\epsilon h }\\dfrac{t^{v}_{i}}{|h|} , t^{*v})$$\n",
    "\n",
    "We have tag vectors for all songs in _FILE_SONG_BOOL_TAGS and tag frequencies in _FILE_TAG_FREQ_FILE. Tag Frequencies are used to calculate IDF scores. We caluclate TF-IDF vector for the playlist of a user and then we caluclate content based similarity score for all other users using the equation mentioned above. Finally we pick top 1000 users and maintain their scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2B) Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_ratio_train_test = 0.8\n",
    "split_ratio_train = 0.2\n",
    "user_list = []\n",
    "neighbours_list = []\n",
    "scores_list = [] \n",
    "\n",
    "# reading the main File\n",
    "frame = pd.read_csv(_FILE_MAIN)\n",
    "frame.drop([\"Playtime\", \"Album\", \"Match\" , \"Listeners\", \"Playcount\", \"Duration\", \"Tags\"], axis=1, inplace=\"True\")\n",
    "\n",
    "# reading the tags related file:\n",
    "## Songs Based Scoring\n",
    "frameSong = pd.read_csv(_FILE_SONG_BOOL_TAGS)\n",
    "frameSong[\"Songs\"] = frame[\"Songs\"]\n",
    "groupedSongs = frameSong.groupby(by=\"Songs\")\n",
    "groupsSongs = groupedSongs.groups\n",
    "full_len = frameSong.shape[0]\n",
    "### Users based grouping \n",
    "groupedUsers = frame.groupby(by=\"Users\")\n",
    "groupsUsers=groupedUsers.groups\n",
    "\n",
    "\n",
    "# Using the dict for storing the IDF vector\n",
    "df_dict = pd.read_csv(_FILE_TAG_FREQ_FILE)\n",
    "IDF_DICT = [math.log10((full_len/df_dict['Frequency'][i])) for i in xrange(df_dict.shape[0])]\n",
    "\n",
    "def findmagnitude(input):\n",
    "    return math.sqrt(sum(input[i]*input[i] for i in range(len(input))))\n",
    "\n",
    "def normalizefunction(input):\n",
    "    listmagnitude =  findmagnitude(input)\n",
    "    if  listmagnitude == 0:\n",
    "        return  input\n",
    "    return [ float(input[i])/listmagnitude  for i in range(len(input)) ]\n",
    "\n",
    "def cosinefunction(input1, input2):\n",
    "    return sum(input1[i]*input2[i] for i in range(len(input1)))\n",
    "\n",
    "def TF_IDF_generator(input):\n",
    "    return_list = []\n",
    "    for i in xrange(len(input)):\n",
    "        if input[i]==0:\n",
    "            val = 0\n",
    "        else:\n",
    "            val = (math.log10(input[i])+1)* IDF_DICT[i]\n",
    "        return_list.append(val)\n",
    "    return return_list\n",
    "\n",
    "def normalizeByPlaylistLength(tf_idf_vector, playlist_length):\n",
    "    normalized_vec=[(value/float(playlist_length)) for value in tf_idf_vector]\n",
    "    return normalized_vec\n",
    "\n",
    "# calculating the user content based on the history\n",
    "def Calculate_content_based_score(username):\n",
    "    print username\n",
    "    historyList = []\n",
    "    user_group = groupedUsers.get_group(username)\n",
    "    #print user_group.Songs\n",
    "    Full_list = user_group.Songs.tolist()\n",
    "    content_start_index = len(Full_list)- int(math.ceil(split_ratio_train_test*len(Full_list)))\n",
    "    content_end_index  = content_start_index + int((split_ratio_train)* (math.ceil(split_ratio_train_test*len(Full_list))))\n",
    "    Train_list = Full_list[content_start_index:content_end_index]\n",
    "    #print Train_list\n",
    "    for tracks in Train_list:\n",
    "        tags_len = len(groupedSongs.get_group(tracks).Tags)\n",
    "        tags_list = groupedSongs.get_group(tracks).Tags.tolist()\n",
    "        historyListString =  str(tags_list[0]).split(\"$\")\n",
    "        historytempvec = []\n",
    "        for itr in historyListString:\n",
    "            historytempvec.append(int(itr))\n",
    "        historyList.append (historytempvec)\n",
    "    TF_IDF_vector_list=[]\n",
    "    for tag_vector in historyList:\n",
    "        TF_IDF_listvec = TF_IDF_generator(tag_vector)\n",
    "        TF_IDF_vector_list.append(TF_IDF_listvec)\n",
    "\n",
    "    TF_IDF_history = [sum(x) for x in zip(*TF_IDF_vector_list)]\n",
    "    UserList_normalized = normalizeByPlaylistLength(TF_IDF_history,len(Train_list))\n",
    "    UserList_normalized=normalizefunction(UserList_normalized)\n",
    "    ScoringDict = collections.defaultdict(list)\n",
    "    for allusers in groupsUsers.keys():\n",
    "        print allusers\n",
    "        if allusers == username:\n",
    "            continue\n",
    "        #print \"Printing the tracks for this user\"\n",
    "        for tracks in groupedUsers.get_group(allusers).Songs.tolist():\n",
    "            if tracks in ScoringDict:\n",
    "                continue\n",
    "            songtempvec = []\n",
    "            #tags_len = len(groupedSongs.get_group(tracks).Tags)\n",
    "            tags_list = groupedSongs.get_group(tracks).Tags.tolist()\n",
    "            songlistString =  str(tags_list[0]).split(\"$\")\n",
    "            for  index,itr in enumerate(songlistString):\n",
    "                  songtempvec.append(int(itr))\n",
    "            TF_IDF_songvec = TF_IDF_generator(songtempvec)\n",
    "            song_normalized = normalizefunction(TF_IDF_songvec)\n",
    "            scoring_val = cosinefunction(UserList_normalized, song_normalized)\n",
    "            ScoringDict[tracks]= scoring_val\n",
    "    reversesorteditems = sorted(ScoringDict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    count = 0\n",
    "    similar_neighbors=[]\n",
    "    similarity_scores=[]\n",
    "    for  items in reversesorteditems:\n",
    "        similar_neighbors.append(str(items[0]))\n",
    "        similarity_scores.append(str(items[1]))                       \n",
    "        count = count + 1\n",
    "    #return similarity_scores\n",
    "    return similar_neighbors, similarity_scores\n",
    "\n",
    "def calculate_for_all(userlist):\n",
    "    for allusers in userlist:\n",
    "        user_list = []\n",
    "        user_list.append(allusers)\n",
    "        #score_list = Calculate_content_based_score(allusers)\n",
    "        similar_neighbors,similarity_scores =  Calculate_content_based_score(allusers)\n",
    "        df = pd.DataFrame(data={\"Neighbours\": similar_neighbors, \"Similarities\": similarity_scores }, \n",
    "                 columns=[\"Neighbours\", \"Similarities\"])  \n",
    "        df.to_csv(str(allusers)+\"CONTENT_SIM.csv\", sep=',')\n",
    "calculate_for_all(_USER_LIST)\n",
    "print \"Content Based Similarity Scores Computed...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3A) Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used tracks general popularity. The assumption of our scoring scheme for numerical values\n",
    "is that if a feature was actually relevant for the selection of tracks, the spread and variance of the values will be low. Given a history $h$ and a feature value $f_{t_{i}}$ for a track $t_{i}$ in the history, we therefore first compute the mean $\\mu$ and standard deviation  of the observed values. If there are too few values available  some might be unknown  or the standard deviation exceeds some threshold , the score is 0. Otherwise, given the feature value $f^{*}_{t}$ for a target track\n",
    "$t^{*}$ we use the value of the probability density function of a Gaussian distribution as a score, where  and  are computed based on the value distributions in the history:\n",
    "\n",
    "$$score_{numfeature}(h, t^{*}) = \\dfrac{1}{\\sigma_{h}^{\\sqrt{2\\pi}}}  e^{- \\dfrac{(f^{*}_{t}-\\mu_{j})^2}{2\\sigma^{2}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3B) Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monkeyhacker\n",
      "badboy495\n",
      "sonnycorleones\n",
      "heavydirtysoul_\n",
      "Garry_Drezden\n",
      "Numerical Features Scores Computed...\n"
     ]
    }
   ],
   "source": [
    "split_ratio_train_test = 0.8\n",
    "split_ratio_train = 0.2\n",
    "frame = pd.read_csv(_FILE_MAIN)\n",
    "frame.drop([\"Playtime\", \"Album\", \"Match\"], axis=1, inplace=\"True\")\n",
    "\n",
    "grouped1 = frame.groupby(by=\"Songs\")\n",
    "groups1 = grouped1.groups\n",
    "\n",
    "groupedusers =frame.groupby(by=\"Users\")\n",
    "groupsusers = groupedusers.groups\n",
    "\n",
    "groupedartist = frame.groupby(by='Artists')\n",
    "groupsartist = groupedartist.groups\n",
    "\n",
    "user_list=[]\n",
    "song_list=[]\n",
    "listener_count = []\n",
    "duration_count = []\n",
    "artist_list = []\n",
    "SongsList = []\n",
    "Artist_scoring_ratio = 0.2\n",
    "\n",
    "neighbours_list = []\n",
    "scores_list = []\n",
    "\n",
    "def calculate_numerical_val(num_feature, mean, variance):\n",
    "    return (1/math.sqrt(2*math.pi* variance))* math.exp(-1*(math.pow((num_feature-mean),2))/(2* variance))\n",
    "\n",
    "def numerical_feature_based_score(userlist):\n",
    "    for username in userlist:\n",
    "        print username\n",
    "        user_list = []\n",
    "        user_list.append(username)\n",
    "        historyList = []\n",
    "        user_group = groupedusers.get_group(username)\n",
    "        #print user_group.Songs\n",
    "        Full_list = user_group.Songs.tolist()\n",
    "        content_start_index = len(Full_list)- int(math.ceil(split_ratio_train_test*len(Full_list)))\n",
    "        content_end_index  = content_start_index + int((split_ratio_train)*(math.ceil(split_ratio_train_test*len(Full_list))))\n",
    "        Train_list = Full_list[content_start_index:content_end_index]\n",
    "        numlist = []\n",
    "        songdict = {}\n",
    "        for itr, row in groupedusers.get_group(username).iterrows():\n",
    "            if math.isnan(row[\"Playcount\"]):\n",
    "                numlist.append(0.0)\n",
    "            else:\n",
    "                numlist.append(row[\"Playcount\"])\n",
    "        mean_num = np.mean(numlist)\n",
    "        var_num = np.cov(numlist)   \n",
    "        for allusers in groupsusers.keys():\n",
    "            if allusers == username:\n",
    "                continue\n",
    "            for itr1,row1 in groupedusers.get_group(allusers).iterrows():\n",
    "                if math.isnan(row1[\"Playcount\"]):\n",
    "                    songdict[row1[\"Songs\"]] = calculate_numerical_val(0.0,mean_num,var_num)\n",
    "                else:\n",
    "                    songdict[row1[\"Songs\"]] = calculate_numerical_val(row1[\"Playcount\"],mean_num,var_num)\n",
    "        reversesorteditems = sorted(songdict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        count = 0\n",
    "        similar_neighbors=[]\n",
    "        similarity_scores=[]\n",
    "        for  items in reversesorteditems:\n",
    "            similar_neighbors.append(str(items[0]))\n",
    "            similarity_scores.append(str(items[1]))                       \n",
    "            count = count + 1\n",
    "#         scores_list = []\n",
    "#         neighbours_list = []\n",
    "#         neighbours_list.append(similar_neighbors)\n",
    "#         scores_list.append(similarity_scores)\n",
    "     \n",
    "        df = pd.DataFrame(data={\"Neighbours\": similar_neighbors, \"Similarities\": similarity_scores }, \n",
    "                 columns=[\"Neighbours\", \"Similarities\"])  \n",
    "        df.to_csv(str(username) + \"NUMERICAL_FEATURES.csv\", sep=',')\n",
    "numerical_feature_based_score(_USER_LIST)\n",
    "print \"Numerical Features Scores Computed...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Personal Preferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4A) Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Personal preferences, e.g., regarding favorite artists, can influence what people add to their playlists. In our experiments we therefore also include an example of a personalized scoring scheme. We base the method on content features- tags. To build a long-term content-based profile for user u\n",
    "with the recent listening history h, we first compute an averaged TF-IDF vector $CP_{h}$ from entire histoory of  u. For a track $t^{*}$ with a corresponding TF-IDF vector $t_{*v}$, the personalized score ($score_{contentpers}$) is then based on a weighted combination of the similarity of $t^{*}$ with the current history $score_{content}(h, t^{*})$ and the similarity of $t_{v}$ with the long-term profile $CP_{h}$, i.e.\n",
    "$$score_{contentpers}(h, t^{*}) = \\alpha  score_{content}(h, t^{*}) + (1  \\alpha)  sim_{cosine}(CP_{h}, t^{v})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4B) Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_ratio_train_test = 0.8\n",
    "split_ratio_train = 0.3\n",
    "user_list = []\n",
    "neighbours_list = []\n",
    "scores_list = [] \n",
    "Preference_factor = 0.4\n",
    "\n",
    "# reading the main File\n",
    "frame = pd.read_csv(_FILE_MAIN)\n",
    "frame.drop([\"Playtime\", \"Album\", \"Match\" , \"Listeners\", \"Playcount\", \"Duration\", \"Tags\"], axis=1, inplace=\"True\")\n",
    "\n",
    "# reading the tags related file:\n",
    "## Songs Based Scoring\n",
    "frameSong = pd.read_csv(_FILE_SONG_BOOL_TAGS)\n",
    "frameSong[\"Songs\"] = frame[\"Songs\"]\n",
    "groupedSongs = frameSong.groupby(by=\"Songs\")\n",
    "groupsSongs = groupedSongs.groups\n",
    "full_len = frameSong.shape[0]\n",
    "### Users based grouping \n",
    "groupedUsers = frame.groupby(by=\"Users\")\n",
    "groupsUsers=groupedUsers.groups\n",
    "\n",
    "# Using the dict for storing the IDF vector\n",
    "df_dict = pd.read_csv(_FILE_TAG_FREQ_FILE)\n",
    "IDF_DICT = [math.log10((full_len/df_dict['Frequency'][i])) for i in xrange(df_dict.shape[0])]\n",
    "\n",
    "def findmagnitude(input):\n",
    "    return math.sqrt(sum(input[i]*input[i] for i in range(len(input))))\n",
    "\n",
    "def normalizefunction(input):\n",
    "    listmagnitude =  findmagnitude(input)\n",
    "    if  listmagnitude == 0:\n",
    "        return  input\n",
    "    return [ float(input[i])/listmagnitude  for i in range(len(input)) ]\n",
    "\n",
    "def cosinefunction(input1, input2):\n",
    "    return sum(input1[i]*input2[i] for i in range(len(input1)))\n",
    "\n",
    "def TF_IDF_generator(input):\n",
    "    return_list = []\n",
    "    for i in xrange(len(input)):\n",
    "        if input[i]==0:\n",
    "            val = IDF_DICT[i]\n",
    "        else:\n",
    "            val = math.log10(input[i]+1)* IDF_DICT[i]\n",
    "        return_list.append(val)\n",
    "    return return_list\n",
    "\n",
    "def normalizeByPlaylistLength(tf_idf_vector, playlist_length):\n",
    "    normalized_vec=[(value/float(playlist_length)) for value in tf_idf_vector]\n",
    "    return normalized_vec\n",
    "\n",
    "# calculating the user content based on the history\n",
    "def User_preference_based_score(username):\n",
    "    historyList = []\n",
    "    historyFullList = []\n",
    "    user_group = groupedUsers.get_group(username)\n",
    "    #print user_group.Songs\n",
    "    Full_list_complete = user_group.Songs.tolist()\n",
    "    content_start_index = len(Full_list_complete)- int(math.ceil(split_ratio_train_test*len(Full_list_complete)))\n",
    "    Full_train_list = Full_list_complete[content_start_index:]\n",
    "    content_end_index  = content_start_index + int((split_ratio_train)* (math.ceil(split_ratio_train_test*len(Full_list_complete))))\n",
    "    Train_list = Full_list[content_start_index:content_end_index]\n",
    "    #print Train_list\n",
    "    for tracks in Train_list:\n",
    "        tags_len = len(groupedSongs.get_group(tracks).Tags)\n",
    "        tags_list = groupedSongs.get_group(tracks).Tags.tolist()\n",
    "        historyListString =  str(tags_list[0]).split(\"$\")\n",
    "        historytempvec = []\n",
    "        for itr in historyListString:\n",
    "            historytempvec.append(int(itr))\n",
    "        historyList.append (historytempvec)\n",
    "    for tracks in Full_train_list:\n",
    "        tags_len = len(groupedSongs.get_group(tracks).Tags)\n",
    "        tags_list = groupedSongs.get_group(tracks).Tags.tolist()\n",
    "        historyListFullString =  str(tags_list[0]).split(\"$\")\n",
    "        historyFulltempvec = []\n",
    "        for itr in historyFullListString:\n",
    "            historyFulltempvec.append(int(itr))\n",
    "        historyFullList.append (historyFulltempvec)\n",
    "    for tag_vector in historyList:\n",
    "        TF_IDF_listvec = TF_IDF_generator(tag_vector)\n",
    "        TF_IDF_vector_list.append(TF_IDF_listvec)\n",
    "\n",
    "    TF_IDF_history = [sum(x) for x in zip(*TF_IDF_vector_list)]\n",
    "    UserList_normalized = normalizeByPlaylistLength(TF_IDF_history,len(Train_list))\n",
    "    UserList_normalized=normalizefunction(UserList_normalized)\n",
    "\n",
    "    for tag_vector in historyFullList:\n",
    "        TF_IDF_listvec = TF_IDF_generator(tag_vector)\n",
    "        TF_IDF_vector_list.append(TF_IDF_listvec)\n",
    "\n",
    "    TF_IDF_history = [sum(x) for x in zip(*TF_IDF_vector_list)]\n",
    "    UserFullList_normalized = normalizeByPlaylistLength(TF_IDF_history,len(Train_list))\n",
    "    UserFullList_normalized=normalizefunction(UserFullList_normalized)\n",
    "    ScoringDict = collections.defaultdict(list)\n",
    "    for allusers in groupsUsers.keys():\n",
    "        print allusers\n",
    "        if allusers == username:\n",
    "            continue\n",
    "        for tracks in groupedUsers.get_group(allusers).Songs.tolist():\n",
    "             if tracks in ScoringDict:\n",
    "                 continue\n",
    "             songtempvec = []\n",
    "             tags_list = groupedSongs.get_group(tracks).Tags.tolist()\n",
    "             songlistString =  str(tags_list[0]).split(\"$\")\n",
    "             for  index,itr in enumerate(songlistString):\n",
    "                  songtempvec.append(int(itr))\n",
    "             TF_IDF_songvec = TF_IDF_generator(songtempvec)\n",
    "             song_normalized = normalizefunction(TF_IDF_songvec)\n",
    "             ScoringDict[tracks]= Preference_factor * cosinefunction(UserList_normalized, song_normalized)+ (1- Preference_factor)* cosinefunction(UserFullList_normalized, song_normalized)\n",
    "    \n",
    "    reversesorteditems = sorted(ScoringDict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    count = 0\n",
    "    similar_neighbors=[]\n",
    "    similarity_scores=[]\n",
    "    for  items in reversesorteditems:\n",
    "       if count == 300:\n",
    "           break\n",
    "       similar_neighbors.append(str(items[0]))\n",
    "       similarity_scores.append(str(items[1]))                         \n",
    "       count = count + 1      \n",
    "    return similar_neighbors, similarity_scores\n",
    "\n",
    "def calculate_for_all():\n",
    "    userlist = ['monkeyhacker','badboy495','sonnycorleones','heavydirtysoul_','Garry_Drezden']\n",
    "    for allusers in userlist:\n",
    "        user_list = []\n",
    "        user_list.append(allusers)\n",
    "        print \"Printing the length of the users\"\n",
    "        print allusers\n",
    "        print \"Ending the users\"\n",
    "        #score_list = Calculate_content_based_score(allusers)\n",
    "        neb_list,score_list =  User_preference_based_score(allusers)\n",
    "        neighbours_list = []\n",
    "        scores_list = []\n",
    "        neighbours_list.append(neb_list)\n",
    "        scores_list.append(score_list)\n",
    "        df = pd.DataFrame(\n",
    "    #data={\"Users\": user_list, \"Neighbours\": neighbours_list, \"Similarities\": scores_list },\n",
    "        data={\"Users\": user_list,\"Neighbours\": neighbours_list, \"Similarities\": scores_list },\n",
    "          columns=[\"Users\",\"Neighbours\" \"Similarities\"])\n",
    "        df.to_csv(str(allusers)+\"USER_PREF.csv\", sep=',')\n",
    "calculate_for_all()\n",
    "print \"Personal Preferences Scores Computed...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Artists History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5A) Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find playlists that contain tracks of only one single artist. If a history h has only one artist A, we apply an additional heuristic and compute a special artist score (AS) for tracks by A. As  described in the paper, we used a baseline AS value that is generally higher than the combined scores as computed of previous scores. This ensures that the playlist continuation starts with the right artist. The tracks of artist A are then sorted according to their popularity, and we use a position-based decay function that assigns a lower AS value to less popular tracks of the artist and apply a minimum popularity threshold. The tracks of the other artists receive the usual combined score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5B) Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['monkeyhacker']\n",
      "['']\n",
      "['']\n",
      "['badboy495']\n",
      "['']\n",
      "['']\n",
      "['sonnycorleones']\n",
      "['']\n",
      "['']\n",
      "['heavydirtysoul_']\n",
      "['']\n",
      "['']\n",
      "['Garry_Drezden']\n",
      "['Tears in Heaven', 'Layla', 'Cocaine', 'Wonderful Tonight', 'Change The World', 'I Shot The Sheriff', 'Lay Down Sally', \"My Father's Eyes\", \"Knockin' On Heaven's Door\", 'Old Love', 'Running on Faith', 'Let It Grow', 'Before You Accuse Me', 'Promises', 'Let It Rain', 'Bad Love', 'Hey Hey', 'Forever Man', 'Lonely Stranger', 'San Francisco Bay Blues', \"Walkin' Blues\", 'Malted Milk', 'Sunshine Of Your Love', \"I Can't Stand It\", 'Badge', 'Layla (unplugged)', 'Blue Eyes Blue', 'Bell Bottom Blues', 'River of Tears', \"She's Waiting\", 'White Room', \"It's In The Way That You Use It\", 'Layla (Unplugged version)', \"Nobody Knows You When You're Down & Out\", 'Crossroads', 'Hello Old Friend', 'Riding With The King', 'If I Had Possession Over Judgement Day', 'Me and the Devil Blues', \"Nobody Knows You When You're Down and Out\", 'The Core', \"We're All The Way\", 'Autumn Leaves', 'Key To The Highway', 'Double Trouble', 'Worried Life Blues', 'Mainline Florida', 'Ten Long Years', 'Layla (Live)', 'Run Back To Your Side', 'Cajun Moon', 'Knock on Wood', \"Can't Let You Do It\", 'See What Love Can Do', 'Call Me The Breeze', 'Never Make You Cry', 'Someday (feat. Mark Knopfler)', 'Double Trouble (Live)', 'Mustang Sally', 'Lies (feat. John Mayer)', 'Rock And Roll Records (feat. Tom Petty)', 'Sensitive Kind (feat. Don White)', 'I Wanna Make Love to You', 'She\\xc2\\xb4s Waiting', 'Laydown Sally', 'Old Love (Live)', 'Christmas Tears - Live', 'I Shot The Sheriff (Live from Crossroads 2010)', 'Coca\\xc3\\xafne', 'Badget', 'Alberta']\n",
      "[3779199.0, 3247136.0, 2811226.0, 2544019.0, 1179012.0, 1166434.0, 853175.0, 737108.0, 610480.0, 566475.0, 541114.0, 513117.0, 497913.0, 487769.0, 458074.0, 432666.0, 402765.0, 401721.0, 385228.0, 353105.0, 318656.0, 306930.0, 288942.0, 282141.0, 279532.0, 276248.0, 271992.0, 271950.0, 260667.0, 254688.0, 254555.0, 240731.0, 231269.0, 225930.0, 213937.0, 198913.0, 167921.0, 149858.0, 139085.0, 136854.0, 116987.0, 116857.0, 111741.0, 85461.0, 80064.0, 74408.0, 73100.0, 49971.0, 46358.0, 43318.0, 30035.0, 27678.0, 23901.0, 22721.0, 20363.0, 18001.0, 13478.0, 12972.0, 12660.0, 11673.0, 9322.0, 7922.0, 7094.0, 3826.0, 3772.0, 3727.0, 3327.0, 2322.0, 1691.0, 1227.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "split_ratio_train_test = 0.8\n",
    "split_ratio_train = 0.2\n",
    "frame = pd.read_csv(_FILE_MAIN)\n",
    "frame.drop([\"Playtime\", \"Album\", \"Match\"], axis=1, inplace=\"True\")\n",
    "\n",
    "grouped1 = frame.groupby(by=\"Songs\")\n",
    "groups1 = grouped1.groups\n",
    "\n",
    "groupedusers =frame.groupby(by=\"Users\")\n",
    "groupsusers = groupedusers.groups\n",
    "\n",
    "groupedartist = frame.groupby(by='Artists')\n",
    "groupsartist = groupedartist.groups\n",
    "\n",
    "user_list=[]\n",
    "song_list=[]\n",
    "listener_count = []\n",
    "duration_count = []\n",
    "artist_list = []\n",
    "SongsList = []\n",
    "Artist_scoring_ratio = 0.2\n",
    "\n",
    "neighbours_list = []\n",
    "scores_list = []\n",
    "\n",
    "def artist_scoring_based_score(userlist):\n",
    "    for username in userlist:\n",
    "        songdict = {}\n",
    "        artist_list = []\n",
    "        user_list=[]\n",
    "        user_list.append(username)\n",
    "        user_group=groupedusers.get_group(username)\n",
    "        Full_list = user_group.Artists.tolist()\n",
    "        content_start_index = len(Full_list)- int(math.ceil(split_ratio_train_test*len(Full_list)))\n",
    "        content_end_index  = content_start_index + int((split_ratio_train)* (math.ceil(split_ratio_train_test*len(Full_list))))\n",
    "        Train_list = Full_list[content_start_index:content_end_index]\n",
    "        artist_list.extend(Full_list)\n",
    "        artist_dict = collections.Counter(artist_list)\n",
    "        similar_neighbors=[]\n",
    "        similarity_scores=[]\n",
    "        artist_name = max(artist_dict, key=artist_dict.get)\n",
    "        if artist_dict[artist_name] > Artist_scoring_ratio * len(artist_list):\n",
    "            for itr, row in groupedartist.get_group(artist_name).iterrows():\n",
    "                if math.isnan(row[\"Playcount\"]):\n",
    "                    songdict[row[\"Songs\"]] = 0.0\n",
    "                else:\n",
    "                    songdict[row[\"Songs\"]] = row[\"Playcount\"]\n",
    "            reversesorteditems = sorted(songdict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "            count = 0\n",
    "\n",
    "            for  items in reversesorteditems:\n",
    "                similar_neighbors.append(str(items[0]))\n",
    "                similarity_scores.append(items[1])\n",
    "        else:\n",
    "                similar_neighbors.append(\"\")\n",
    "                similarity_scores.append(\"\")\n",
    "        print user_list\n",
    "        neighbours_list = []\n",
    "        scores_list = []\n",
    "        neighbours_list.append(similar_neighbors)\n",
    "        scores_list.append(similarity_scores)\n",
    "        df = pd.DataFrame(data={\"Users\": user_list, \"Neighbours\": neighbours_list, \"Similarities\": scores_list }, \n",
    "                 columns=[\"Users\", \"Neighbours\", \"Similarities\"]) \n",
    "        df.to_csv(str(username) + \"ARTIST_SCORE.csv\", sep=',')\n",
    "artist_scoring_based_score(_USER_LIST)\n",
    "print \"Artist History Scores Computed...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Combining the Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6A) Theory\n",
    "Depending on the available data, various combinations of scores of different types can be used. As per the paper, we tested different configurations using a weighted scoring scheme. Given a set of scoring functions S, the score is \n",
    "\n",
    "$$score_{overall}(h, t^{*}) = \\Sigma_{score_{i}\\epsilon S}score_{i}(h, t^{*}) * w_{i}$$ \n",
    "\n",
    "where $w_{i}$ is a configurable weight term for each $score_{i}\\epsilon S$. The selection of the scores as well as their corresponding weights depend on the goals that should be achieved. We tested different values for the weight of each score and selected the best ones (listed below) with respect to accuracy. The weights are in accordance with the paper.\n",
    "\n",
    "<table>\n",
    "<tr><td>Scoring</td><td>Weight</td></tr>\n",
    "<tr><td>kNN300</td><td>1.0</td></tr>\n",
    "<tr><td>ART=single-artist heuristic</td><td>4.0</td></tr>\n",
    "<tr><td>CP=ContentPers.</td><td>0.3, $\\alpha$=0.6</td></tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B) Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Next track optimiztion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7A) Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7B) Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Evaluation of Faceted Track Scoring\n",
    "**Accuracy and MRR**  \n",
    "Hit Rate: We compare the hidden user playlist and the suggested playlist. 20 percent of the user playlist is hidden. and we suggest 100. Let's say we 10 songs are hidden and there are 5 of the hidden songs in suggested songs then accuracy is 5/10=0.5. We report the average of the accuracy for all the users.\n",
    "\n",
    "**Diversity and Coherence - Tags**  \n",
    "Inverse Intra List Similarity (ILS): Intra List Similarity is the sum of the pairwise similarity cosine of TF-IDF vectors of the songs in the suggested songlist by the algorithm. Inverse ILS is the reciprocal of ILS. \n",
    "Tags Overlap: This is the Jaccard similarity measure of the set of tags of user playlist and set of tags of suggested playlist.\n",
    "\n",
    "**Diversity and Coherence - Artist**  \n",
    "Artists Overlap: This is the Jaccard similarity measure of the set of artists of user playlist and set of artists of suggested playlist.  \n",
    "\n",
    "**Diversity and Coherence - Numerical Features Method**  \n",
    "Differences in mean and SD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
